ğŸ“Œ Projeto: Reasoning Lab (GUI Multi-Model com llama.cpp)
ğŸ¯ Objetivo

Construir uma aplicaÃ§Ã£o Linux com interface grÃ¡fica moderna que execute um pipeline hierÃ¡rquico de LLMs locais usando llama.cpp, com modelos obtidos via Ollama registry, estruturados em trÃªs papÃ©is:

Reasoning model (small)

Judge model (validador/interprete)

Final model (sÃ­ntese final)

A aplicaÃ§Ã£o deve:

Permitir seleÃ§Ã£o visual dos modelos

Executar pipeline sequencial

Mostrar discussÃ£o interna opcionalmente

Exibir apenas a resposta final ao concluir

Permitir experimentos de robustez interpretativa

ğŸ§  Arquitetura Conceitual
Pipeline HierÃ¡rquico
User Prompt
    â†“
Reasoning Model
    â†“
Judge Model
    â†“
Final Model
    â†“
User Output

Papel de cada modelo
1ï¸âƒ£ Reasoning Model (menor, rÃ¡pido)

Resolve passo a passo

Pode errar

Foco: geraÃ§Ã£o de raciocÃ­nio estruturado

2ï¸âƒ£ Judge Model

Avalia consistÃªncia

Detecta erros de modelagem

Pode aceitar, corrigir ou reformular

3ï¸âƒ£ Final Model

Consolida a decisÃ£o do judge

Produz saÃ­da limpa e definitiva

Remove ruÃ­do de raciocÃ­nio

ğŸ–¥ï¸ Interface (GUI)

Implementada em PySide6 (Qt6).

Estrutura de NavegaÃ§Ã£o
1ï¸âƒ£ Home Page

Branding

BotÃ£o "Start"

2ï¸âƒ£ Model Selection Page

Dropdown para:

Reasoning

Judge

Final

Modelos listados via ollama list

PersistÃªncia futura possÃ­vel

3ï¸âƒ£ Execution Page

Campo de prompt

BotÃ£o "Run"

Toggle:

â€œMostrar raciocÃ­nioâ€

Ãrea de output:

Streaming em tempo real

Ou apenas resposta final


reasoning_lab_gui/
â”‚
â”œâ”€â”€ main.py
â”‚
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ home_page.py
â”‚   â”œâ”€â”€ model_select_page.py
â”‚   â””â”€â”€ run_page.py
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ controller.py
â”‚   â””â”€â”€ llama_runner.py
â”‚
â”œâ”€â”€ assets/
â”‚
â””â”€â”€ models/   (opcional)




ğŸ”§ Backend TÃ©cnico
ExecuÃ§Ã£o de modelos

llama.cpp via subprocess

subprocess.Popen com streaming de stdout

ExecuÃ§Ã£o em QThread para nÃ£o bloquear UI

Download de modelos

Via:

ollama pull <model>


Uso de:


ollama list --json


Para popular dropdown automaticamente.


ğŸ§© Prompt Engineering
Reasoning Prompt Template


You are a reasoning model.
Solve step by step.

Question:
{user_question}

Answer:



Judge Prompt Template




You are a judge model.

Question:
{user_question}

Reasoning attempt:
{reasoning_output}

Evaluate correctness and explain flaws if any.




Final Prompt Template



You are the final synthesis model.

Question:
{user_question}

Judge analysis:
{judge_output}

Provide the final consolidated answer clearly and correctly.




ğŸ“¦ DependÃªncias
Sistema

Linux

Python 3.10+

llama.cpp compilado

Ollama instalado



Python
pip install pyside6


Opcional:

pip install psutil
pip install rich



âš™ï¸ Componentes TÃ©cnicos CrÃ­ticos
1ï¸âƒ£ Threading

Uso de:

QThread

Signals:

update (stream parcial)

finished (resultado final)

Sem threading â†’ UI trava.

2ï¸âƒ£ Streaming de stdout

subprocess.Popen(...)
for line in process.stdout:
    callback(line)

Permite:

VisualizaÃ§Ã£o da â€œdiscussÃ£oâ€

SimulaÃ§Ã£o de debate entre modelos

3ï¸âƒ£ Controle de ExibiÃ§Ã£o

Se: toggle == True
â†’ mostra raciocÃ­nio ao vivo

Se:
toggle == False


â†’ suprime discussÃ£o e mostra apenas output final

ğŸ§ª O Que Este Projeto EstÃ¡ Tentando Fazer

NÃ£o Ã© apenas uma GUI para LLM.

Ele Ã©:

Um laboratÃ³rio de epistemologia computacional.

Objetivos experimentais:

Detectar overthinking

Detectar template overactivation

Analisar propagaÃ§Ã£o de erro hierÃ¡rquico

Testar robustez semÃ¢ntica entre modelos

Ã‰ uma plataforma experimental para investigar raciocÃ­nio artificial.

